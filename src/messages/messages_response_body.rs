use crate::macros::impl_display_for_serialize;
use crate::messages::{
    ClaudeModel, Content, Role, StopReason, StopSequence, Usage,
};

/// The response body for the Messages API.
///
/// See also [the Messages API](https://docs.anthropic.com/claude/reference/messages_post).
#[derive(Debug, Clone, PartialEq, serde::Serialize, serde::Deserialize)]
pub struct MessagesResponseBody {
    /// Unique object identifier.
    ///
    /// The format and length of IDs may change over time.
    pub id: String,
    /// Object type.
    ///
    /// For Messages, this is always "message".
    #[serde(rename = "type")]
    pub _type: String,
    /// Conversational role of the generated message.
    ///
    /// This will always be "assistant".
    pub role: Role,
    /// Content generated by the model.
    ///
    /// This is an array of content blocks, each of which has a type that determines its shape. Currently, the only type in responses is "text".
    pub content: Content,
    /// The model that handled the request.
    pub model: ClaudeModel,
    /// The reason that we stopped.
    ///
    /// This may be one of the following values:
    ///
    /// "end_turn": the model reached a natural stopping point
    /// "max_tokens": we exceeded the requested max_tokens or the model's maximum
    /// "stop_sequence": one of your provided custom stop_sequences was generated
    /// Note that these values are different from those in /v1/complete, where end_turn and stop_sequence were not differentiated.
    ///
    /// In non-streaming mode this value is always non-null. In streaming mode, it is null in the message_start event and non-null otherwise.
    pub stop_reason: Option<StopReason>,
    /// Which custom stop sequence was generated, if any.
    ///
    /// This value will be a non-null string if one of your custom stop sequences was generated.
    pub stop_sequence: Option<StopSequence>,
    /// Billing and rate-limit usage.
    ///
    /// Anthropic's API bills and rate-limits by token counts, as tokens represent the underlying cost to our systems.
    ///
    /// Under the hood, the API transforms requests into a format suitable for the model. The model's output then goes through a parsing stage before becoming an API response. As a result, the token counts in usage will not match one-to-one with the exact visible content of an API request or response.
    ///
    /// For example, output_tokens will be non-zero, even for an empty string response from Claude.
    pub usage: Usage,
}

impl_display_for_serialize!(MessagesResponseBody);
